<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>üé§ Voice to Text (ElevenLabs - Fixed)</title>
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      max-width: 800px;
      margin: 50px auto;
      padding: 30px;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      border-radius: 15px;
      box-shadow: 0 20px 40px rgba(0,0,0,0.3);
    }
    
    h1 {
      text-align: center;
      text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
      margin-bottom: 30px;
    }
    
    .controls {
      display: flex;
      justify-content: center;
      gap: 15px;
      margin-bottom: 30px;
    }
    
    button {
      padding: 12px 24px;
      font-size: 16px;
      font-weight: bold;
      border: none;
      border-radius: 25px;
      cursor: pointer;
      transition: all 0.3s ease;
      box-shadow: 0 4px 15px rgba(0,0,0,0.2);
    }
    
    #start {
      background: linear-gradient(45deg, #4CAF50, #45a049);
      color: white;
    }
    
    #start:hover:not(:disabled) {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(0,0,0,0.3);
    }
    
    #stop {
      background: linear-gradient(45deg, #f44336, #da190b);
      color: white;
    }
    
    #stop:hover:not(:disabled) {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(0,0,0,0.3);
    }
    
    button:disabled {
      background: #666;
      cursor: not-allowed;
      transform: none;
    }
    
    .output-section {
      background: rgba(255,255,255,0.1);
      border-radius: 15px;
      padding: 20px;
      margin: 20px 0;
      backdrop-filter: blur(10px);
    }
    
    #output {
      background: rgba(255,255,255,0.9);
      color: #333;
      padding: 20px;
      border-radius: 10px;
      min-height: 60px;
      margin: 15px 0;
      font-size: 16px;
      line-height: 1.5;
      box-shadow: inset 0 2px 5px rgba(0,0,0,0.1);
    }
    
    .status-section {
      display: flex;
      align-items: center;
      gap: 10px;
      margin: 15px 0;
    }
    
    #status {
      font-weight: bold;
      font-size: 16px;
    }
    
    .recording-indicator {
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background: #ff4444;
      animation: pulse 1s infinite;
      display: none;
    }
    
    .recording-indicator.active {
      display: block;
    }
    
    @keyframes pulse {
      0% { opacity: 1; transform: scale(1); }
      50% { opacity: 0.5; transform: scale(1.1); }
      100% { opacity: 1; transform: scale(1); }
    }
    
    .info {
      background: rgba(255,255,255,0.1);
      border-radius: 10px;
      padding: 15px;
      margin-top: 20px;
      font-size: 14px;
      line-height: 1.4;
    }
    
    .debug-info {
      background: rgba(0,0,0,0.2);
      border-radius: 10px;
      padding: 15px;
      margin-top: 20px;
      font-family: 'Courier New', monospace;
      font-size: 12px;
      max-height: 200px;
      overflow-y: auto;
    }
  </style>
</head>
<body>
  <h1>üé§ Voice to Text Transcription</h1>
  
  <div class="controls">
    <button id="start">üéôÔ∏è Start Recording</button>
    <button id="stop" disabled>‚èπÔ∏è Stop Recording</button>
  </div>
  
  <div class="output-section">
    <h3>üìù Transcription Result:</h3>
    <div id="output">Ready to transcribe your voice...</div>
  </div>
  
  <div class="status-section">
    <strong>Status:</strong> 
    <span id="status">Ready to record</span>
    <div class="recording-indicator" id="recordingIndicator"></div>
  </div>
  
  <div class="info">
    <strong>üìã Instructions:</strong><br>
    1. Click "Start Recording" and allow microphone access<br>
    2. Speak clearly for 5-15 seconds<br>
    3. Click "Stop Recording" to process<br>
    4. Wait for transcription result
  </div>
  
  <div class="debug-info" id="debugInfo">
    <strong>üîç Debug Log:</strong><br>
  </div>

  <script>
    let mediaRecorder;
    let audioChunks = [];
    const startBtn = document.getElementById("start");
    const stopBtn = document.getElementById("stop");
    const output = document.getElementById("output");
    const status = document.getElementById("status");
    const recordingIndicator = document.getElementById("recordingIndicator");
    const debugInfo = document.getElementById("debugInfo");

    function addDebugLog(message) {
      const timestamp = new Date().toLocaleTimeString();
      debugInfo.innerHTML += `[${timestamp}] ${message}<br>`;
      debugInfo.scrollTop = debugInfo.scrollHeight;
    }

    startBtn.addEventListener("click", async () => {
      try {
        addDebugLog("üé§ Requesting microphone access...");
        status.textContent = "Requesting microphone access...";
        
        // Request microphone access with optimal settings
        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 48000,
            channelCount: 1 // Mono audio for speech
          } 
        });

        addDebugLog("‚úÖ Microphone access granted");

        // Check supported MIME types and use the best available
        let mimeType = 'audio/webm;codecs=opus';
        if (!MediaRecorder.isTypeSupported(mimeType)) {
          mimeType = 'audio/webm';
          if (!MediaRecorder.isTypeSupported(mimeType)) {
            mimeType = 'audio/wav';
            if (!MediaRecorder.isTypeSupported(mimeType)) {
              mimeType = 'audio/mp4';
            }
          }
        }

        addDebugLog(`üéµ Using MIME type: ${mimeType}`);

        // Create MediaRecorder
        const options = mimeType ? { mimeType } : {};
        mediaRecorder = new MediaRecorder(stream, options);

        audioChunks = [];

        mediaRecorder.ondataavailable = (event) => {
          addDebugLog(`üìä Audio chunk received: ${event.data.size} bytes`);
          if (event.data.size > 0) {
            audioChunks.push(event.data);
          }
        };

        mediaRecorder.onstop = async () => {
          addDebugLog("üõë Recording stopped, processing...");
          
          const audioBlob = new Blob(audioChunks, { 
            type: mediaRecorder.mimeType || 'audio/webm' 
          });
          
          addDebugLog(`üìÅ Audio blob created: ${audioBlob.size} bytes, type: ${audioBlob.type}`);

          // Create FormData with correct field name
          const formData = new FormData();
          formData.append("audio", audioBlob, "recording.webm");

          try {
            status.textContent = "üîÑ Uploading and transcribing...";
            output.textContent = "Processing audio with ElevenLabs API...";

            addDebugLog("üöÄ Sending to backend...");

            const response = await fetch("http://localhost:5001/api/chatbot/transcribe", {
              method: "POST",
              body: formData,
            });

            addDebugLog(`üì° Response status: ${response.status}`);

            if (!response.ok) {
              const errorText = await response.text();
              addDebugLog(`‚ùå Response error: ${errorText}`);
              throw new Error(`HTTP ${response.status}: ${errorText}`);
            }

            const data = await response.json();
            addDebugLog(`‚úÖ Response data: ${JSON.stringify(data, null, 2)}`);

            if (data.success && data.data) {
              const transcribedText = data.data.text || "No text detected";
              output.innerHTML = `
                <strong>Transcribed Text:</strong><br>
                "${transcribedText}"<br><br>
                <small>
                  <strong>Language:</strong> ${data.data.language || 'Unknown'}<br>
                  <strong>Model:</strong> ${data.data.model_used || 'ElevenLabs'}<br>
                  <strong>Confidence:</strong> ${data.data.confidence || 'N/A'}
                </small>
              `;
              status.textContent = "‚úÖ Transcription complete!";
              addDebugLog(`üéØ Final transcription: "${transcribedText}"`);
            } else {
              throw new Error(data.error || data.message || "Transcription failed");
            }

          } catch (error) {
            addDebugLog(`‚ùå Transcription error: ${error.message}`);
            output.innerHTML = `
              <strong style="color: #ff4444;">‚ùå Error:</strong><br>
              ${error.message}<br><br>
              <small>Please check the debug log below for more details.</small>
            `;
            status.textContent = "‚ùå Transcription failed";
          }

          // Clean up microphone stream
          stream.getTracks().forEach(track => {
            track.stop();
            addDebugLog("üîá Microphone track stopped");
          });

          recordingIndicator.classList.remove('active');
        };

        mediaRecorder.onerror = (event) => {
          addDebugLog(`üî¥ MediaRecorder error: ${event.error}`);
          status.textContent = "‚ùå Recording error: " + event.error;
        };

        // Start recording
        addDebugLog("‚ñ∂Ô∏è Starting recording...");
        mediaRecorder.start(1000); // Collect data every second
        
        startBtn.disabled = true;
        stopBtn.disabled = false;
        status.textContent = "üî¥ Recording in progress...";
        output.textContent = "üéôÔ∏è Listening... Speak now!";
        recordingIndicator.classList.add('active');

      } catch (error) {
        addDebugLog(`‚ùå Error accessing microphone: ${error.message}`);
        status.textContent = "‚ùå Microphone error: " + error.message;
        output.textContent = "Please check microphone permissions and try again.";
      }
    });

    stopBtn.addEventListener("click", () => {
      if (mediaRecorder && mediaRecorder.state === "recording") {
        addDebugLog("‚èπÔ∏è User stopped recording");
        mediaRecorder.stop();
        startBtn.disabled = false;
        stopBtn.disabled = true;
        status.textContent = "‚è≥ Processing recording...";
        recordingIndicator.classList.remove('active');
      }
    });

    // Check microphone availability on load
    window.addEventListener('load', async () => {
      try {
        const devices = await navigator.mediaDevices.enumerateDevices();
        const audioInputs = devices.filter(device => device.kind === 'audioinput');
        addDebugLog(`üé§ Available audio inputs: ${audioInputs.length}`);
        
        if (audioInputs.length === 0) {
          status.textContent = "‚ùå No microphone detected";
          startBtn.disabled = true;
        } else {
          addDebugLog("‚úÖ System ready for recording");
        }
      } catch (error) {
        addDebugLog(`‚ö†Ô∏è Could not check microphone availability: ${error.message}`);
      }
    });
  </script>
</body>
</html>